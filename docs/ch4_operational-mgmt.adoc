== Operational Management

With the successful deployment of OpenShift, the following section demonstrates how to confirm proper functionality of the Red Hat OpenShift Container Platform.

=== Gathering hostnames
With all of the steps that occur during the installation of OpenShift, it is possible to lose track of the names of the instances in the recently deployed environment. One option to get these hostnames is to browse to the `Azure Resource Group` dashboard and select `Overview`. The filter shows all instances relating to the reference architecture deployment.

To help facilitate the `Operational Management` Chapter the following hostnames will be used.


* master1
* master2
* master3
* infranode1
* infranode2
* infranode3
* node01
* node02
* node03


=== Running Diagnostics

Perform the following steps from the first master node.

To run diagnostics, `SSH` into the  master node (master1), via the bastion host.

[subs=+quotes]
----
$ *ssh username@resourcegroupb*
$ *ssh username@master1*
$ *sudo -i*
----

Connectivity to the first master node (master1.region.cloudapp.azure.com) as the `root` user should have been established. Run the diagnostics that are included as part of the install.

[subs=+quotes]
----
# *oadm diagnostics*
[Note] Determining if client configuration exists for client/cluster diagnostics
Info:  Successfully read a client config file at '/root/.kube/config'
Info:  Using context for cluster-admin access: 'default/sysdeseng-westus-cloudapp-azure-com:8443/system:admin'
[Note] Performing systemd discovery

[Note] Running diagnostic: ConfigContexts[default/sysdeseng-westus-cloudapp-azure-com:8443/system:admin]
       Description: Validate client config context is complete and has connectivity

Info:  The current client config context is 'default/sysdeseng-westus-cloudapp-azure-com:8443/system:admin':
       The server URL is 'https://sysdeseng.westus.cloudapp.azure.com:8443'
       The user authentication is 'system:admin/sysdeseng-westus-cloudapp-azure-com:8443'
       The current project is 'default'
       Successfully requested project list; has access to project(s):
         [default gsw kube-system logging management-infra openshift openshift-infra]

[Note] Running diagnostic: DiagnosticPod
       Description: Create a pod to run diagnostics from the application standpoint


       [Note] Running diagnostic: PodCheckDns
              Description: Check that DNS within a pod works as expected

       [Note] Summary of diagnostics execution (version v3.5.5.5):
       [Note] Warnings seen: 0
       [Note] Errors seen: 0

[Note] Running diagnostic: NetworkCheck
       Description: Create a pod on all schedulable nodes and run network diagnostics from the application standpoint

       [Note] Running diagnostic: CheckExternalNetwork
              Description: Check that external network is accessible within a pod

       [Note] Running diagnostic: CheckNodeNetwork
              Description: Check that pods in the cluster can access its own node.

       [Note] Running diagnostic: CheckPodNetwork
              Description: Check pod to pod communication in the cluster. In case of ovs-subnet network plugin, all pods should be able to communicate with each other and in case of multitenant network plugin, pods in non-global projects should be isolated and pods in global projects should be able to access any pod in the cluster and vice versa.

       [Note] Running diagnostic: CheckServiceNetwork
              Description: Check pod to service communication in the cluster. In case of ovs-subnet network plugin, all pods should be able to communicate with all services and in case of multitenant network plugin, services in non-global projects should be isolated and pods in global projects should be able to access any service in the cluster.

       [Note] Running diagnostic: CollectNetworkInfo
              Description: Collect network information in the cluster.

       [Note] Summary of diagnostics execution (version v3.5.5.5):
       [Note] Warnings seen: 0


       [Note] Running diagnostic: CheckNodeNetwork
              Description: Check that pods in the cluster can access its own node.

       [Note] Running diagnostic: CheckPodNetwork
              Description: Check pod to pod communication in the cluster. In case of ovs-subnet network plugin, all pods should be able to communicate with each other and in case of multitenant network plugin, pods in non-global projects should be isolated and pods in global projects should be able to access any pod in the cluster and vice versa.

       [Note] Running diagnostic: CheckServiceNetwork
              Description: Check pod to service communication in the cluster. In case of ovs-subnet network plugin, all pods should be able to communicate with all services and in case of multitenant network plugin, services in non-global projects should be isolated and pods in global projects should be able to access any service in the cluster.

       [Note] Running diagnostic: CollectNetworkInfo
              Description: Collect network information in the cluster.

       [Note] Summary of diagnostics execution (version v3.5.5.5):
       [Note] Warnings seen: 0


       [Note] Running diagnostic: CheckNodeNetwork
              Description: Check that pods in the cluster can access its own node.

       [Note] Running diagnostic: CheckPodNetwork
              Description: Check pod to pod communication in the cluster. In case of ovs-subnet network plugin, all pods should be able to communicate with each other and in case of multitenant network plugin, pods in non-global projects should be isolated and pods in global projects should be able to access any pod in the cluster and vice versa.

       [Note] Running diagnostic: CheckServiceNetwork
              Description: Check pod to service communication in the cluster. In case of ovs-subnet network plugin, all pods should be able to communicate with all services and in case of multitenant network plugin, services in non-global projects should be isolated and pods in global projects should be able to access any service in the cluster.

       [Note] Running diagnostic: CollectNetworkInfo
              Description: Collect network information in the cluster.

       [Note] Summary of diagnostics execution (version v3.5.5.5):
       [Note] Warnings seen: 0

[Note] Skipping diagnostic: AggregatedLogging
       Description: Check aggregated logging integration for proper configuration
       Because: No LoggingPublicURL is defined in the master configuration

[Note] Running diagnostic: ClusterRegistry
       Description: Check that there is a working Docker registry

[Note] Running diagnostic: ClusterRoleBindings
       Description: Check that the default ClusterRoleBindings are present and contain the expected subjects

Info:  clusterrolebinding/cluster-readers has more subjects than expected.

       Use the `oadm policy reconcile-cluster-role-bindings` command to update the role binding to remove extra subjects.

Info:  clusterrolebinding/cluster-readers has extra subject {ServiceAccount management-infra management-admin    }.
Info:  clusterrolebinding/cluster-readers has extra subject {ServiceAccount default router    }.

Info:  clusterrolebinding/self-provisioners has more subjects than expected.

       Use the `oadm policy reconcile-cluster-role-bindings` command to update the role binding to remove extra subjects.

Info:  clusterrolebinding/self-provisioners has extra subject {ServiceAccount management-infra management-admin    }.

[Note] Running diagnostic: ClusterRoles
       Description: Check that the default ClusterRoles are present and contain the expected permissions

[Note] Running diagnostic: ClusterRouterName
       Description: Check there is a working router

[Note] Running diagnostic: MasterNode
       Description: Check if master is also running node (for Open vSwitch)

WARN:  [DClu3004 from diagnostic MasterNode@openshift/origin/pkg/diagnostics/cluster/master_node.go:164]
       Unable to find a node matching the cluster server IP.
       This may indicate the master is not also running a node, and is unable
       to proxy to pods over the Open vSwitch SDN.

[Note] Skipping diagnostic: MetricsApiProxy
       Description: Check the integrated heapster metrics can be reached via the API proxy
       Because: The heapster service does not exist in the openshift-infra project at this time,
       so it is not available for the Horizontal Pod Autoscaler to use as a source of metrics.

[Note] Running diagnostic: NodeDefinitions
       Description: Check node records on master

WARN:  [DClu0003 from diagnostic NodeDefinition@openshift/origin/pkg/diagnostics/cluster/node_definitions.go:112]
       Node master1 is ready but is marked Unschedulable.
       This is usually set manually for administrative reasons.
       An administrator can mark the node schedulable with:
           oadm manage-node master1 --schedulable=true

       While in this state, pods should not be scheduled to deploy on the node.
       Existing pods will continue to run until completed or evacuated (see
       other options for 'oadm manage-node').

WARN:  [DClu0003 from diagnostic NodeDefinition@openshift/origin/pkg/diagnostics/cluster/node_definitions.go:112]
       Node master2 is ready but is marked Unschedulable.
       This is usually set manually for administrative reasons.
       An administrator can mark the node schedulable with:
           oadm manage-node master2 --schedulable=true

       While in this state, pods should not be scheduled to deploy on the node.
       Existing pods will continue to run until completed or evacuated (see
       other options for 'oadm manage-node').

WARN:  [DClu0003 from diagnostic NodeDefinition@openshift/origin/pkg/diagnostics/cluster/node_definitions.go:112]
       Node master3 is ready but is marked Unschedulable.
       This is usually set manually for administrative reasons.
       An administrator can mark the node schedulable with:
           oadm manage-node master3 --schedulable=true

       While in this state, pods should not be scheduled to deploy on the node.
       Existing pods will continue to run until completed or evacuated (see
       other options for 'oadm manage-node').

[Note] Running diagnostic: ServiceExternalIPs
       Description: Check for existing services with ExternalIPs that are disallowed by master config

[Note] Running diagnostic: AnalyzeLogs
       Description: Check for recent problems in systemd service logs

Info:  Checking journalctl logs for 'atomic-openshift-node' service
Info:  Checking journalctl logs for 'docker' service

[Note] Running diagnostic: MasterConfigCheck
       Description: Check the master config file

WARN:  [DH0005 from diagnostic MasterConfigCheck@openshift/origin/pkg/diagnostics/host/check_master_config.go:52]
       Validation of master config file '/etc/origin/master/master-config.yaml' warned:
       assetConfig.loggingPublicURL: Invalid value: "": required to view aggregated container logs in the console
       assetConfig.metricsPublicURL: Invalid value: "": required to view cluster metrics in the console
       auditConfig.auditFilePath: Required value: audit can now be logged to a separate file

[Note] Running diagnostic: NodeConfigCheck
       Description: Check the node config file

Info:  Found a node config file: /etc/origin/node/node-config.yaml

[Note] Running diagnostic: UnitStatus
       Description: Check status for related systemd units

[Note] Summary of diagnostics execution (version v3.5.5.5):
[Note] Warnings seen: 5
[Note] Errors seen: 0
----

NOTE: The warnings will not cause issues in the environment

Based on the results of the diagnostics, actions can be taken to alleviate any issues.

=== Checking the Health of ETCD

This section focuses on the `ETCD` cluster. It describes the different commands to ensure the cluster is healthy. The internal `DNS` names of the nodes running `ETCD` must be used.

`SSH` into the first master node (master1). This must be done via bastion host.
Using the output of the command `hostname` issue the `etcdctl` command to confirm that the cluster is healthy.

[subs=+quotes]
----
$ ssh azure-user@master1.southeastasia.cloudapp.azure.com
$ sudo -i
----


[subs=+quotes]
----
# *etcdctl -C https://master1.southeastasia.cloudapp.azure.com:2379 --ca-file /etc/etcd/ca.crt --cert-file=/etc/origin/master/master.etcd-client.crt --key-file=/etc/origin/master/master.etcd-client.key cluster-health*
member 82c895b7b0de4330 is healthy: got healthy result from https://10.20.1.106:2379
member c8e7ac98bb93fe8c is healthy: got healthy result from https://10.20.3.74:2379
member f7bbfc4285f239ba is healthy: got healthy result from https://10.20.2.157:2379
----

NOTE: In this configuration the `ETCD` services are distributed among the OpenShift master nodes.

=== Default Node Selector
As explained in section 2.12.4 node labels are an important part of the OpenShift environment. By default of the reference architecture installation, the default node selector is set to "role=apps" in `/etc/origin/master/master-config.yaml` on all of the master nodes.  This configuration parameter is set during the installation of OpenShift on all masters.

`SSH` into the first master node (master1) to verify the `defaultNodeSelector` is defined.

[subs=+quotes]
----
# vi /etc/origin/master/master-config.yaml
...omitted...
projectConfig:
  defaultNodeSelector: "role=app"
  projectRequestMessage: ""
  projectRequestTemplate: ""
...omitted...
----

NOTE: If making any changes to the master configuration then the master API service must be restarted or the configuration change will not take place. Any changes and the subsequent restart must be done on all masters.

=== Management of Maximum Pod Size
Quotas are set on ephemeral volumes within pods to prohibit a pod from becoming to large and impacting the node. There are three places where sizing restrictions should be set. When persistent volume claims are not set a pod has the ability to grow as large as the underlying filesystem will allow. The required modifcations are set by automatically.


*Openshift Volume Quota*

At launch time a script creates a `xfs` partition on the block device, adds an entry in fstab, and mounts the volume with the option of `gquota`. If `gquota` is not set the OpenShift node will not be able to start with the `perFSGroup` parameter defined below. This disk and configuration is done on the master, infrastructure, and application nodes.

`SSH` into the first infrastructure node (infranode1) to verify the entry exists within fstab.

[subs=+quotes]
----
# vi /etc/fstab
/dev/sdc1 /var/lib/origin/openshift.local.volumes xfs gquota 0 0
----

*Docker Storage Setup*

The `docker-storage-setup` file is created at launch time by the bash script. This file tells the Docker service to use a specific volume group for containers.  The extra Docker storage options ensures that a container can grow no larger than 3G.  Docker storage setup is performed on all master, infrastructure, and application nodes.

`SSH` into the first infrastructure node (infranode1) to verify `/etc/sysconfig/docker-storage-setup` matches the information below.

[subs=+quotes]
----
# vi /etc/sysconfig/docker-storage-setup
DEVS=/dev/sdb1
VG=docker-vol
DATA_SIZE=95%VG
EXTRA_DOCKER_STORAGE_OPTIONS="--storage-opt dm.basesize=3G"
----

*OpenShift Emptydir Quota*

During installation a value for `perFSGroup` is set within the node configuration. The `perFSGroup` setting restricts the ephemeral emptyDir volume from growing larger than 512Mi.  This empty dir quota is done on the master, infrastructure, and application nodes. 

`SSH` into the first infrastructure node (infranode1) to verify `/etc/origin/node/node-config.yml` matches the information below.

[subs=+quotes]
----
# vi /etc/origin/node/node-config.yml
...omitted...
volumeConfig:
  localQuota:
     perFSGroup: 512Mi
----

=== Yum Repositories
In section 2.3 Required Channels the specific repositories for a successful OpenShift installation were defined.  All systems except for the bastion host should have the same subscriptions. To verify subscriptions match those defined in Required Channels perfom the following. The repositories below are enabled during the rhsm-repos playbook during the installation. The installation will be unsuccessful if the repositories are missing from the system.

image::images/repolist.png[]


=== Console Access

This section will cover logging into the OpenShift Container Platform management console via
the GUI and the CLI. After logging in via one of these methods applications can then be deployed and managed.

==== Log into GUI console and deploy an application

Perform the following steps from the local workstation.

Open a browser and access  https://resourcegroupname.region.cloudapp.azure.com/console.
The resourcegroupname is given in the ARM template, and region is the Azure zone selected during install.
When logging into the OpenShift web interface, use the user login and password specified during the launch of the ARM template.

To deploy an application, click on the `New Project` button. Provide a `Name` and click `Create`. Next, deploy the `jenkins-ephemeral` instant app by clicking the corresponding box. Accept the defaults and click `Create`. Instructions along with a URL will be provided for how to access the application on the next screen. Click `Continue to Overview` and bring up the management page for the application. Click on the link provided and access the application to confirm functionality.

==== Log into CLI and Deploy an Application

Perform the following steps from the local workstation.

Install the `oc client` by visiting the public URL of the OpenShift deployment. For example, https://resourcegroupname.region.cloudapp.azure.com/console/command-line and click latest release. When directed to https://access.redhat.com, login with the valid Red Hat customer credentials and download the client relevant to the current workstation. Follow the instructions located on the production documentation site for https://docs.openshift.com/container-platform/3.5/cli_reference/get_started_cli.html[getting started with the cli].

A token is required to login to OpenShift. The token is presented on the https://resourcegroupname.region.cloudapp.azure.com/console/command-line page. Click the click to show token hyperlink and perform the following on the workstation in which the oc client was installed.

[subs=+quotes]
----
$ *oc login https://resourcegroupname.region.cloudapp.azure.com --token=fEAjn7LnZE6v5SOocCSRVmUWGBNIIEKbjD9h-Fv7p09*
----

After the oc client is configured, create a new project and deploy an application.

[subs=+quotes]
----
$ *oc new-project test-app*

$ *oc new-app https://github.com/openshift/cakephp-ex.git --name=php*
--> Found image 2997627 (7 days old) in image stream "php" in project "openshift" under tag "5.6" for "php"

    Apache 2.4 with PHP 5.6
    -----------------------
    Platform for building and running PHP 5.6 applications

    Tags: builder, php, php56, rh-php56

    * The source repository appears to match: php
    * A source build using source code from https://github.com/openshift/cakephp-ex.git will be created
      * The resulting image will be pushed to image stream "php:latest"
    * This image will be deployed in deployment config "php"
    * Port 8080/tcp will be load balanced by service "php"
      * Other containers can access this service through the hostname "php"

--> Creating resources with label app=php ...
    imagestream "php" created
    buildconfig "php" created
    deploymentconfig "php" created
    service "php" created
--> Success
    Build scheduled, use 'oc logs -f bc/php' to track its progress.
    Run 'oc status' to view your app.


$ *oc expose service php*
route "php" exposed
----

<<<

Display the status of the application.

[subs=+quotes]
----
$ *oc status*
In project test-app on server https://resourcegroupname.region.cloudapp.azure.com

http://test-app.apps.13.93.162.100.nip.io to pod port 8080-tcp (svc/php)
  dc/php deploys istag/php:latest <- bc/php builds https://github.com/openshift/cakephp-ex.git with openshift/php:5.6
    deployment #1 deployed about a minute ago - 1 pod

1 warning identified, use 'oc status -v' to see details.
----

Access the application by accessing the URL provided by `oc status`.  The CakePHP application should be visible now.

=== Explore the Environment

==== List Nodes and Set Permissions

Try to run the following command, it should fail.

[subs=+quotes]
----
# *oc get nodes --show-labels*
Error from server: User "syseng-admin" cannot list all nodes in the cluster
----

The reason it is failing is because the permissions for that user are incorrect. Get the username and configure the permissions.

[subs=+quotes]
----
$ *oc whoami*
----

Once the username has been established, log back into a master node and enable the appropriate permissions for the user. Perform the following step from the first master (master1).

[subs=+quotes]
----
# *oadm policy add-cluster-role-to-user cluster-admin syseng-admin*
----

<<<

Attempt to list the nodes again and show the labels.

[subs=+quotes]
----
# *oc get nodes --show-labels*
NAME          STATUS                     AGE
infranode1    Ready                      16d
infranode2    Ready                      16d
infranode3    Ready                      16d
master1       Ready,SchedulingDisabled   16d
master2       Ready,SchedulingDisabled   16d
master3       Ready,SchedulingDisabled   16d
node01        Ready                      16d
node02        Ready                      16d
node03        Ready                      16d

----

==== List Router and Registry

List the router and registry by changing to the `default` project.

NOTE: Perform the following steps from the local workstation.

[subs=+quotes]
----
# *oc project default*
# *oc get all*
NAME                         REVISION        DESIRED       CURRENT   TRIGGERED BY
dc/docker-registry           1               1             1         config
dc/router                    1               2             2         config
NAME                         DESIRED         CURRENT       AGE
rc/docker-registry-1         1               1             10m
rc/router-1                  2               2             10m
NAME                         CLUSTER-IP      EXTERNAL-IP   PORT(S)                   AGE
svc/docker-registry          172.30.243.63   <none>        5000/TCP                  10m
svc/kubernetes               172.30.0.1      <none>        443/TCP,53/UDP,53/TCP     20m
svc/router                   172.30.224.41   <none>        80/TCP,443/TCP,1936/TCP   10m
NAME                         READY           STATUS        RESTARTS                  AGE
po/docker-registry-1-2a1ho   1/1             Running       0                         8m
po/router-1-1g84e            1/1             Running       0                         8m
po/router-1-t84cy            1/1             Running       0                         8m

----

Observe the output of `oc get all`

<<<

==== Explore the Docker Registry
The OpenShift Ansible playbooks configure three infrastructure nodes that have one registry running. In order to understand the configuration and mapping process of the registry pods, the command `oc describe` is used.
`oc describe` details how registries are configured and mapped to the Azure Blob's for storage. Using `oc describe` should help explain how HA works in this environment.

NOTE: Perform the following steps from the local workstation.

[subs=+quotes]
----
$ *oc describe svc/docker-registry*
Name:			docker-registry
Namespace:		default
Labels:			docker-registry=default
Selector:		docker-registry=default
Type:			ClusterIP
IP:			172.30.110.31
Port:			5000-tcp	5000/TCP
Endpoints:		172.16.4.2:5000,172.16.4.3:5000
Session Affinity:	ClientIP
No events.
----

Notice that the registry has two `endpoints` listed. Each of those `endpoints` represents a Docker container. The `ClusterIP` listed is the actual ingress point for the registries.

==== Explore Docker Storage

This section will explore the Docker storage on an infrastructure node.

The example below can be performed on any node but for this example the infrastructure node(infranode1) is used.

The output below verifies docker storage is not using a loop back device. If it was in loopback, the output would
list the loopback file. As the below output does not contain the word loopback, the docker daemon is working in the
optimal way.

[subs=+quotes]
----
$ docker info
Containers: 2
 Running: 2
 Paused: 0
 Stopped: 0
Images: 4
Server Version: 1.10.3
Storage Driver: devicemapper
 Pool Name: docker--vol-docker--pool
 Pool Blocksize: 524.3 kB
 Base Device Size: 3.221 GB
 Backing Filesystem: xfs
 Data file:
 Metadata file:
 Data Space Used: 1.221 GB
 Data Space Total: 25.5 GB
 Data Space Available: 24.28 GB
 Metadata Space Used: 307.2 kB
 Metadata Space Total: 29.36 MB
 Metadata Space Available: 29.05 MB
 Udev Sync Supported: true
 Deferred Removal Enabled: true
 Deferred Deletion Enabled: true
 Deferred Deleted Device Count: 0
 Library Version: 1.02.107-RHEL7 (2016-06-09)
Execution Driver: native-0.2
Logging Driver: json-file
Plugins:
 Volume: local
 Network: bridge null host
 Authorization: rhel-push-plugin
Kernel Version: 3.10.0-327.10.1.el7.x86_64
Operating System: Employee SKU
OSType: linux
Architecture: x86_64
Number of Docker Hooks: 2
CPUs: 2
Total Memory: 7.389 GiB
Name: ip-10-20-3-46.azure.internal
ID: XDCD:7NAA:N2S5:AMYW:EF33:P2WM:NF5M:XOLN:JHAD:SIHC:IZXP:MOT3
WARNING: bridge-nf-call-iptables is disabled
WARNING: bridge-nf-call-ip6tables is disabled
Registries: registry.access.redhat.com (secure), docker.io (secure)
----

==== Explore the Azure Load Balancers

As mentioned earlier in the document two `Load Balancers` have been created. The purpose of this section is to encourage exploration of the `LBs` that were created.

NOTE: Perform the following steps from the `Azure` web console.

On the main `Azure` dashboard, click on `Resource Groups` icon. Then select the resource group that corresponds with the OpenShift Deployment, and then find the Load Balancers within the resource group. Select the `AppLB` load balancer and on the `Description` page note the `Port Configuration` and how it is configured. That is for the OpenShift application traffic.
There should be three master instances running with a `Status` of `Ok`. Next check the `Health Check` tab and the options that were configured.
Further details of the configuration can be viewed by exploring the Azure ARM templates to see exactly what was configured.

==== Explore the Azure Resource Group

As mentioned earlier in the document an Azure Resource Group was created. The purpose of this section is to encourage exploration of the `Resource Group` that was created.

NOTE: Perform the following steps from the `Azure` web console.

On the main Microsoft Azure console, click on `Resource Group`. Next on the left hand navigation panel select the `Your Resource Groups`.
Select the `Resource Group` recently created and explore the `Summary` tabs. Next, on the right hand navigation panel, explore the `Virtual Machines`, `Storage Accounts`, `Load Balancers`, and `Networks`.
More detail can be looked at with the configuration by exploring the Ansible playbooks and ARM json Files to see exactly what was configured.

=== Testing Failure

In this section, reactions to failure are explored. After a sucessful install and some of the smoke tests noted above have been completed, failure testing is executed.

==== Generate a Master Outage

NOTE: Perform the following steps from the `Azure` web console and the OpenShift public URL.

Log into the `Azure` console.  On the dashboard, click on the `Resource Group` web service and then click `Overview`. Locate the running master2 instance, select it, right click and change the state to `stopped`.

Ensure the console can still be accessed by opening a browser and accessing https://resourcegroupname.region.cloudapp.azure.com. At this point, the cluster is in a degraded state because only 2/3 master nodes are running, but complete funcionality remains.

==== Observe the Behavior of `ETCD` with a Failed Master Node

`SSH` into the first master node (master1) from the bastion. Using the output of the command `hostname` issue the `etcdctl` command to confirm that the cluster is healthy.

[subs=+quotes]
----
$ *ssh user@master1*
$ *sudo -i*
----


[subs=+quotes]
----
# *etcdctl -C https://master1:2379 --ca-file /etc/etcd/ca.crt --cert-file=/etc/origin/master/master.etcd-client.crt --key-file=/etc/origin/master/master.etcd-client.key cluster-health*
failed to check the health of member 82c895b7b0de4330 on https://10.20.2.251:2379: Get https://10.20.1.251:2379/health: dial tcp 10.20.1.251:2379: i/o timeout
member 82c895b7b0de4330 is unreachable: [https://10.20.1.251:2379] are all unreachable
member c8e7ac98bb93fe8c is healthy: got healthy result from https://10.20.3.74:2379
member f7bbfc4285f239ba is healthy: got healthy result from https://10.20.1.106:2379
cluster is healthy
----

Notice how one member of the `ETCD` cluster is now unreachable. Restart master2 by following the same steps in the `Azure` web console as noted above.

==== Generate an Infrastruture Node outage

This section shows what to expect when an infrastructure node fails or is brought down intentionally.

===== Confirm Application Accessibility

NOTE: Perform the following steps from the browser on a local workstation.

Before bringing down an infrastructure node, check behavior and ensure things are working as expected. The goal of testing an infrastructure node outage is to see how the OpenShift routers and registries behave. Confirm the simple application deployed from before is still functional. If it is not, deploy a new version. Access the application to confirm connectivity.
As a reminder, to find the required information to ensure the application is still running, list the projects, change to the project that the application is deployed in, get the status of the application which including the URL and access the application via that URL.

[subs=+quotes]
----
$ oc get projects
NAME               DISPLAY NAME   STATUS
openshift                         Active
openshift-infra                   Active
ttester                           Active
test-app1                         Active
default                           Active
management-infra                  Active

$ oc project test-app1
Now using project "test-app1" on server "https://resourcegroupname.region.cloudapp.azure.com".

$ oc status
In project test-app1 on server https://resourcegroupname.region.cloudapp.azure.com

http://php-test-app1.apps.apps.region.cloudapp.azure.com to pod port 8080-tcp (svc/php-prod)
  dc/php-prod deploys istag/php-prod:latest <-
    bc/php-prod builds https://github.com/openshift/cakephp-ex.git with openshift/php:5.6
    deployment #1 deployed 27 minutes ago - 1 pod

1 warning identified, use 'oc status -v' to see details.
----

Open a browser and ensure the application is still accessible.

===== Confirm Registry Functionality

This section is another step to take before initiating the outage of the infrastructure node to ensure that the registry is functioning properly. The goal is to push to the OpenShift registry.

NOTE: Perform the following steps from a CLI on a local workstation and ensure that the oc client has been configured.

A token is needed so that the Docker registry can be logged into.

[subs=+quotes]
----
# oc whoami -t
feAeAgL139uFFF_72bcJlboTv7gi_bo373kf1byaAT8
----

Pull a new docker image for the purposes of test pushing.

[subs=+quotes]
----
# docker pull fedora/apache
# docker images
----

Capture the registry endpoint. The `svc/docker-registry` shows the endpoint.

[subs=+quotes]
----
# oc status
In project default on server https://resourcegroupname.region.cloudapp.azure.com

svc/docker-registry - 172.30.237.147:5000
  dc/docker-registry deploys docker.io/openshift3/ose-docker-registry:v3.3.0.32
    deployment #2 deployed 51 minutes ago - 1 pods
    deployment #1 deployed 53 minutes ago

svc/kubernetes - 172.30.0.1 ports 443, 53->8053, 53->8053

svc/router - 172.30.144.227 ports 80, 443, 1936
  dc/router deploys docker.io/openshift3/ose-haproxy-router:v3.3.0.32
    deployment #1 deployed 55 minutes ago - 2 pods

View details with 'oc describe <resource>/<name>' or list everything with 'oc get all'.
----

Tag the docker image with the endpoint from the previous step.

[subs=+quotes]
----
# docker tag docker.io/fedora/apache 172.30.110.31:5000/openshift/prodapache
----

Check the images and ensure the newly tagged image is available.

[subs=+quotes]
----
# docker images
----

<<<

Issue a Docker login.

[subs=+quotes]
----
# docker login -u syseng-admin -e syseng-admin -p _7yJcnXfeRtAbJVEaQwPwXreEhlV56TkgDwZ6UEUDWw 172.30.110.31:5000
----

[subs=+quotes]
----
# oadm policy add-role-to-user admin syseng-admin -n openshift
# oadm policy add-role-to-user system:registry syseng-admin
# oadm policy add-role-to-user system:image-builder syseng-admin
----

Push the image to the OpenShift registry now.

[subs=+quotes]
----
# docker push 172.30.110.222:5000/openshift/prodapache
The push refers to a repository [172.30.110.222:5000/openshift/prodapache]
389eb3601e55: Layer already exists
c56d9d429ea9: Layer already exists
2a6c028a91ff: Layer already exists
11284f349477: Layer already exists
6c992a0e818a: Layer already exists
latest: digest: sha256:ca66f8321243cce9c5dbab48dc79b7c31cf0e1d7e94984de61d37dfdac4e381f size: 6186
----

<<<

===== Get Location of Router and Registry.

NOTE: Perform the following steps from the CLI of a local workstation.

Change to the default OpenShift project and check the router and registry pod locations.

[subs=+quotes]
----
$ oc project default
Now using project "default" on server "https://resourcegroupname.region.cloudapp.azure.com".

$ oc get pods
NAME                      READY     STATUS    RESTARTS   AGE
docker-registry-2-gmvdr   1/1       Running   1          21h
router-1-6y5td            1/1       Running   1          21h
router-1-rlcwj            1/1       Running   1          21h

$ oc describe pod docker-registry-2-jueep | grep -i node
Node:		ip-10-30-1-17.azure.internal/10.30.1.17
$ oc describe pod router-1-6y5td | grep -i node
Node:		ip-10-30-1-17.azure.internal/10.30.1.17
$ oc describe pod router-1-rlcwj | grep -i node
Node:		ip-10-30-2-208.azure.internal/10.30.2.208
----

===== Initiate the Failure and Confirm Functionality

NOTE: Perform the following steps from the `Azure` web console and a browser.

Log into the `Azure` console.  On the dashboard, click on the `Resource Group`.
Locate the running infranode1 instance, select it, right click and change the state to `stopped`.
Wait a minute or two for the registry and pod to migrate over to a different infranode.
Check the registry locations and confirm that they are on the same node.

[subs=+quotes]
----
$ oc describe pod docker-registry-2-fw1et | grep -i node
Node:		ip-10-30-2-208.azure.internal/10.30.2.208
----

Follow the procedures above to ensure a Docker image can still be pushed to the registry now that infranode1 is down.

// vim: set syntax=asciidoc:
