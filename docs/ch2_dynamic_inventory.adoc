[[refarch_details]]

<<<

=== Dynamic Inventory
Ansible relies on inventory files and variables to perform playbook runs.
As part of the reference architecture provided Ansible playbooks,
the inventory is created during the boot of the bastion host. The Azure Resource Manager
template passes parameters via a script extension to RHEL on the bastion. On the bastion host
a bastion.sh script generates the inventory file in /etc/ansible/hosts.

[[app-listing]]
.Dynamic Inventory Script within bastion.sh
[source,bash]
----

cat <<EOF > /etc/ansible/hosts
[OSEv3:children]
masters
etcd
nodes

[OSEv3:vars]
debug_level=2
console_port=8443
openshift_node_debug_level="{{ node_debug_level | default(debug_level, true) }}"
openshift_master_debug_level="{{ master_debug_level | default(debug_level, true) }}"
openshift_master_access_token_max_seconds=2419200
openshift_hosted_router_replicas=4
openshift_hosted_registry_replicas=1
openshift_hosted_router_selector='role=app'
openshift_hosted_registry_selector='role=app'
openshift_master_api_port="{{ console_port }}"
openshift_master_console_port="{{ console_port }}"
openshift_override_hostname_check=true
osm_use_cockpit=false
openshift_release=v3.4
azure_resource_group=${RESOURCEGROUP}
rhn_pool_id=${RHNPOOLID}
openshift_install_examples=true
deployment_type=openshift-enterprise
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]

# default selectors for router and registry services
openshift_router_selector='role=infra'
openshift_registry_selector='role=infra'

# Select default nodes for projects
osm_default_node_selector="role=app"
ansible_become=yes
ansible_ssh_user=${AUSERNAME}
remote_user=${AUSERNAME}

openshift_master_default_subdomain=${WILDCARDZONE}.trafficmanager.net
openshift_use_dnsmasq=false
openshift_public_hostname=${RESOURCEGROUP}.trafficmanager.net

openshift_master_cluster_method=native
openshift_master_cluster_hostname=${RESOURCEGROUP}.trafficmanager.net
openshift_master_cluster_public_hostname=${RESOURCEGROUP}.trafficmanager.net

# default storage plugin dependencies to install, by default the ceph and
# glusterfs plugin dependencies will be installed, if available.
osn_storage_plugin_deps=['Azure VHD']

[masters]
master1.${domain} openshift_node_labels="{'role': 'master'}"
master2.${domain} openshift_node_labels="{'role': 'master'}"
master3.${domain}  openshift_node_labels="{'role': 'master'}"

[etcd]
master1.${domain}
master2.${domain}
master3.${domain}

[nodes]
master1.${domain} openshift_node_labels="{'role':'master','zone':'default'}"
master2.${domain} openshift_node_labels="{'role':'master','zone':'default'}"
master3.${domain} openshift_node_labels="{'role':'master','zone':'default'}"
node[01:${NODECOUNT}].${domain} openshift_node_labels="{'role': 'app', 'zone': 'default'}"
infranode1.${domain}  openshift_node_labels="{'role': 'infra', 'zone': 'default'}"
infranode2.${domain}  openshift_node_labels="{'role': 'infra', 'zone': 'default'}"
infranode3.${domain} openshift_node_labels="{'role': 'infra', 'zone': 'default'}"

EOF

----




For the OpenShift installation, The Azure Resource Manager template collects the
needed parameters, creates the virtual machines, and passes the parameters to the virtual
machines, where a node type specific script in bash will take the parameters and
generate the needed playbooks and automation. During this process each VM is assigned
an ansible tag, that allows the playbooks to address the different node types.


For more information see For more information see:
Azure Linux Automation: https://azure.microsoft.com/en-us/blog/automate-linux-vm-customization-tasks-using-customscript-extension/

Ansible Host Inventory: http://docs.ansible.com/ansible/intro_inventory.html

// vim: set syntax=asciidoc:
