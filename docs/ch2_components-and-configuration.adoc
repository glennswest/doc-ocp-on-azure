[[refarch_details]]

== Components and Configuration

This chapter describes the reference architecture environment that is deployed that
enables the configuration of a highly available OpenShift Container Platform 3 environment on Microsoft Azure.

The image below provides a high-level representation of the components within this
reference architecture.  Using Microsoft Azure (Azure), resources are highly
available using a combination of VM placement using Azure Availability Sets, Azure Load Balancer(ALB),
and `Azure VHD` persistent volumes. Instances deployed are given specific roles
to support OpenShift. The Bastion host limits the external access to internal servers by ensuring that
all `SSH` traffic passes through the Bastion host. The master instances host the
OpenShift master components such as etcd and the OpenShift API.  The Application
instances are for users to deploy their containers while the Infrastructure
instances are used for the OpenShift router and registry.  Authentication is managed
by the htpasswd Identity Provider.  OpenShift on `Azure` uses a combination of premium and standard storage.
Storage is used for the filesystem of instances but can also be used for persistent
storage in containers.

The network is configured to leverage two `Azure Load Balancers` for access to
the OpenShift API, OpenShift console, and the OpenShift routers. The first
load balancer is for the OpenShift API and console access originating from outside
of the cluster. The second load balancer is for application access from outside the cluster.
The OpenShift Console, and API may be accessed directly via the automatically created
DNS entry. The application access must be accessed from an externally created domain, that
uses a C record to forward traffic to the application load balancer.

image::images/OSE-on-Azure.png[]

This reference architecture breaks down the deployment into three separate phases.

* Phase 1: Provision the Virtual Machines on `Azure`
* Phase 2: Install OpenShift Compute Platform on `Azure`
* Phase 3: Post deployment activities

For Phase 1, the provisioning of the environment is done using a series of
Azure Resource Manager Templates (ARM) that are provided in the
https://github.com/glennswest/openshift-ansible-contrib/tree/master/reference-architecture/azure-ansible[openshift-ansible-contrib] github repo.
Once the infrastructure is deployed by `ARM`, as the last action, the ARM templates will start
the next phase by running a bash script that starts phase 2.

Phase 2 is the provisioning of OpenShift Container Platform which is done via the
Ansible playbooks installed by the `openshift-ansible-playbooks` rpm package. This is
driven by a set of bash scripts that setup the inventory, setup parameters, and make sure
all the needed playbooks are coordinated. As the last part of phase 2, the router and registry
are deployed.

The last phase, Phase 3, concludes the deployment, which is done manually. This consists
of configuring the DNS entry to point to the application load balancers and by manually
verifying the configuration. This is done by running tools like `oadm diagnostics` and the
systems engineering teams `validation` Ansible playbook.


NOTE: The scripts provided in the github repo are not supported by Red Hat. They merely provide a mechanism that can be used to build out your own infrastructure.

=== Azure Cloud Instance Details

Within this reference environment, the instances are deployed in a single `Azure Region`
which can be selected when running the ARM template.  Although the default region can
be changed, the reference architecture deployment should only be
used in Regions with premium storage. Note that where a customer wishes higher availability, multiple
clusters should be created, and federation should be used. This architecture is emerging and will
be described in future reference architecture.

The master instances for the OpenShift environment are `Standard_DS4_v2`. The node instances
are `Standard_DS4_v2` used for Docker containers and OpenShift. The infrastructure nodes use `Standard_DS4_v2` instances.
The bastion host is a `Standard_DS1_v2`.
Instance sizing can be changed when the ARM template is run which is covered in later chapters.

=== Azure Load Balancer Details

Two load balancers are used in the reference environment. The table below describes the load balancer
`DNS` name, the instances in which the `Azure Load Balancer` is attached, and the port monitored by the load balanacer to state whether an instance is in or out of service.

.Azure Load Balancer
|====
^|ALB ^| Assigned Instances ^| Port

| _resourcegroupname_._region_.cloudapp.azure.com | master1-3 | 443
| _wildcardzone_._publicip_.nip.io | infra-nodes1-3 | 80 and 443
|====

The `openshift-master ALB` utilizes the OpenShift Master API port for communication internally and externally.
The _wildcardzone_._publicip_.nip.io `ALB` uses the public subnets and maps to infrastructure nodes.
The infrastructure nodes run the router pod which then directs traffic directly from the outside world into OpenShift pods with external routes defined.

Note that in order to use the wildcard zone, an external C-Record DNS Entry must be defined, with a wildcard in front.

=== Software Version Details

The following tables provide the installed software versions for the different servers that make up the Red Hat OpenShift highly available reference environment.

.RHEL OSEv3 Details
|====
^|Software ^|Version

|Red Hat Enterprise Linux 7.3 x86_64 | kernel-3.10.0-327
| Atomic-OpenShift{master/clients/node/sdn-ovs/utils} | 3.5.x.x
| Docker | 1.12.x
| Ansible | 2.2.1
|====

=== Required Channels

A subscription to the following channels is required in order to deploy this reference environment's configuration.

.Required Channels - OSEv3 Master and Node Instances
|====
^|Channel ^|Repository Name

| Red Hat Enterprise Linux 7 Server (RPMs) |
rhel-7-server-rpms | Red Hat OpenShift Enterprise 3.5 (RPMs) | rhel-7-server-ose-3.5-rpms
| Red Hat Enterprise Linux 7 Server - Extras (RPMs) | rhel-7-server-extras-rpms
| Red Hat Enterprise Linux 7 Server - Fast Datapath (RPMs) | rhel-7-fast-datapath-rpms

|====

The subscriptions are accessed via a _pool_id_, which is required when launching the ARM
template


=== Prerequisites
This section describes the environment and setup needed to execute the ARM template,
and perform post installation tasks.

==== GitHub Repositories
The code in the `openshift-ansible-contrib` repository referenced below handles the installation of OpenShift
and the accompanying infrastructure. The `openshift-ansible-contrib` repository is not explicitly supported by
Red Hat but the Reference Architecture team performs testing to ensure the code operates as defined and is secure.

https://github.com/openshift/openshift-ansible-contrib/tree/master/reference-architecture/azure-ansible


For this reference architecture, the scripts are accessed and used directly from GitHub.
There is no requirement to download the code, as it's done automatically once the script is started.

=== Azure Subscription

In order to deploy the template, a Azure Subscription is required. A trial subscription is
not recommended, as the Reference Architecture uses significant resources, and the typical
trial subscription does not provide adequate resources.

The deployment of OpenShift requires a user that has the proper permissions by the
 `Azure` administrator. The user must be able to create accounts, storage accounts,
roles, policies, load balancers, and deploy virtual machine instances.
It is helpful to have delete permissions in order to be able to redeploy the environment
while testing.


=== Azure Region Selection

An OpenShift cluster is deployed with-in one Azure Region. In order to get the best possible
availability in Azure, availability sets are implemented.

In Azure, virtual machines (VMs) can be placed in to a logical grouping called an availability set.
When you create VMs within an availability set, the Azure platform distributes the placement of those VMs
across the underlying infrastructure. Should there be a planned maintenance event to the Azure platform or an
underlying hardware / infrastructure fault, the use of availability sets ensures that at least one VM remains
running. The Azure SLA requires two or more VMs within an availability set to allow the distribution of VMs across
the underlying infrastructure.


=== SSH Public and Private Key
SSH Keys are used instead of passwords in the Azure OCP Install. These keys are generated
on the system that will be used to login and manage the system. In addition, they are automatically
distributed by the Azure Resource Management template to all virtual machines
that are created.

In order to use the template, ssh public and private keys are needed. It is important to not apply a passphrase to the key.

==== SSH Key Generation
If SSH keys do not currently exist then it is required to create them. Generate an RSA key pair by typing the following at a shell prompt:

[source,bash]
----
~]$ ssh-keygen -t rsa -N ''
Generating public/private rsa key pair.
Enter file in which to save the key (/home/USER/.ssh/id_rsa):
----

Press Enter to confirm default location.

After this, you will be presented with a message similar to this:

[source,bash]
----
Your identification has been saved in /home/USER/.ssh/id_rsa.
Your public key has been saved in /home/USER/.ssh/id_rsa.pub.
The key fingerprint is:
e7:97:c7:e2:0e:f9:0e:fc:c4:d7:cb:e5:31:11:92:14 USER@penguin.example.com
The key's randomart image is:
+--[ RSA 2048]----+
|             E.  |
|            . .  |
|             o . |
|              . .|
|        S .    . |
|         + o o ..|
|          * * +oo|
|           O +..=|
|           o*  o.|
+-----------------+
----

=== Resource Groups and Resource Group Name
In Azure, resources are grouped together in resource groups. You may choose any resource group
name that is unique for your subscription. Note that multiple resource groups are supported in a region, in addition you may have resource groups in
multiple regions. A resource group may not span multiple regions.



// vim: set syntax=asciidoc:
